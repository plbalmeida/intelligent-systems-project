{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-panama",
   "metadata": {},
   "source": [
    "# Development of Intelligent Computing Systems _ 2022  IME-USP\n",
    "- course [page][4]\n",
    "- ministred by: MSc [Renato Cordeiro Ferreira][1]\n",
    "- student: [Pedro Almeida][3] and [Rodrigo Didier Anderson][2]\n",
    "\n",
    "[1]: https://www.linkedin.com/in/renatocf/\n",
    "[2]: https://www.linkedin.com/in/didier11/\n",
    "[3]: https://www.linkedin.com/in/plbalmeida/\n",
    "[4]: https://www.ime.usp.br/verao/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-friendly",
   "metadata": {},
   "source": [
    "This is the first part of the course project, we will create the training pipeline for a categorization model.\n",
    "\n",
    "More specifically, the goal is to train a model that should receive data related to products and return the best categories for them.\n",
    "\n",
    "\n",
    "- More details about this stage of the project [here][1].\n",
    "- More info about the data can be found [here][2]\n",
    "[1]: https://github.com/didier-rda/intelligent-systems-project/blob/main/training/README.md\n",
    "[2]: https://github.com/didier-rda/intelligent-systems-project/blob/main/data/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-johnson",
   "metadata": {},
   "source": [
    "## Training Pipeline  \n",
    "\n",
    "This training pipeline follows the following steps. \n",
    "\n",
    "For each step, a class was created with the necessary methods to fulfill the respective stage of the pipline.\n",
    "\n",
    "1. **Data extraction** <br>\n",
    "   class: `DataExtractor`\n",
    "   \n",
    "   Loads a dataset with product data from a specified path available in the\n",
    "   environment variable `DATASET_PATH`.\n",
    "   \n",
    "   \n",
    "   \n",
    "2. **Data formatting** <br>\n",
    "   class: `DataFormatter`\n",
    "   \n",
    "   Processes \"query\" feature through CountVectorizer class from scikit-learn, training (70%) and test (30%) sets are generated in sequence.\n",
    "      \n",
    "\n",
    "\n",
    "3. **Modeling** <br>\n",
    "   class: `Modeler`\n",
    "   \n",
    "   Naive Bayes classifier was the chosen model, it's commonly used for testing NLP classification problems. Therefore, the MultinomialNB class from scikit-learn was used, which works well with integer type features generated through CountVectorizer. MultinomialNB is also used for multiple label classification, which is the problem of the present dataset.\n",
    "   \n",
    "\n",
    "\n",
    "4. **Model validation** <br>\n",
    "   class: `ModelValidator`\n",
    "   \n",
    "   Generates metrics about the model accuracy (precision, recall, F1, etc.)\n",
    "   for each category and exports them to a specified path available in the\n",
    "   environment variable `METRICS_PATH`.\n",
    "   \n",
    "   \n",
    "   \n",
    "5. **Model exportation** <br>\n",
    "   class: `ModelExporter`\n",
    "   \n",
    "   Exports a candidate model to a specified path available in the environment variable `MODEL_PATH`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-principle",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "combined-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-injury",
   "metadata": {},
   "source": [
    "# 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.path = os.getenv('DATASET_PATH')\n",
    "        self.data = self.get_data()\n",
    "    \n",
    "    def get_data(self):\n",
    "        return pd.read_csv(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-portable",
   "metadata": {},
   "source": [
    "#  2. Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welsh-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFormatter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = DataExtractor().data\n",
    "    \n",
    "    def data_split(self):\n",
    "        y = self.data['category']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.data['query'], y, test_size=0.3, random_state=123)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def count_vectorizer(self):\n",
    "        count_vectorizer = CountVectorizer() \n",
    "        count_train = count_vectorizer.fit_transform(self.data_split()[0])\n",
    "        count_test = count_vectorizer.transform(self.data_split()[1])\n",
    "        return count_train, count_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gilbert",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intelligent-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeler:\n",
    "    \n",
    "    def classifier(self):    \n",
    "        nb_classifier = MultinomialNB()\n",
    "        nb_classifier.fit(DataFormatter().count_vectorizer()[0], DataFormatter().data_split()[2])\n",
    "        return nb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-relaxation",
   "metadata": {},
   "source": [
    "# 4. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "annoying-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelValidator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Modeler().classifier()\n",
    "    \n",
    "    def prediction(self):\n",
    "        y_pred = self.model.predict(DataFormatter().count_vectorizer()[1])\n",
    "        return y_pred\n",
    "    \n",
    "    def metrics(self):\n",
    "        metrics_report = classification_report(DataFormatter().data_split()[3].values, self.prediction())\n",
    "        f = open(os.getenv('METRICS_PATH'), 'w')\n",
    "        f.write(f'Test set metrics:\\n')\n",
    "        f.write(f'\\n{metrics_report}\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-fancy",
   "metadata": {},
   "source": [
    "# 5. Model exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "defined-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExporter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Modeler().classifier()    \n",
    "        pickle.dump(self.model, open(os.getenv('MODEL_PATH'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e323a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline execution\n",
    "if __name__ == '__main__':\n",
    "    ModelValidator().metrics()\n",
    "    ModelExporter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
