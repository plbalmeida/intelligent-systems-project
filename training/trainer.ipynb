{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rural-panama",
   "metadata": {},
   "source": [
    "# Development of Intelligent Computing Systems _ 2022  IME-USP\n",
    "- course [page][4]\n",
    "- ministred by: MSc [Renato Cordeiro Ferreira][1]\n",
    "- student: [Pedro Almeida][3] and [Rodrigo Didier Anderson][2]\n",
    "\n",
    "[1]: https://www.linkedin.com/in/renatocf/\n",
    "[2]: https://www.linkedin.com/in/didier11/\n",
    "[3]: https://www.linkedin.com/in/plbalmeida/\n",
    "[4]: https://www.ime.usp.br/verao/index.php"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-friendly",
   "metadata": {},
   "source": [
    "This is the first part of the course project, we will create the training pipeline for a categorization model.\n",
    "\n",
    "More specifically, the goal is to train a model that should receive data related to products and return the best categories for them.\n",
    "\n",
    "\n",
    "- More details about this stage of the project [here][1].\n",
    "- More info about the data can be found [here][2]\n",
    "[1]: https://github.com/didier-rda/intelligent-systems-project/blob/main/training/README.md\n",
    "[2]: https://github.com/didier-rda/intelligent-systems-project/blob/main/data/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-johnson",
   "metadata": {},
   "source": [
    "## Training Pipeline  \n",
    "\n",
    "This training pipeline follows the following steps. \n",
    "\n",
    "For each step, a class was created with the necessary methods to fulfill the respective stage of the pipline.\n",
    "\n",
    "1) **Data extraction** <br>\n",
    "   class: `DataExtractor`\n",
    "   \n",
    "   Loads a dataset with product data from a specified path available in the\n",
    "   environment variable `DATASET_PATH`.\n",
    "   \n",
    "   \n",
    "   \n",
    "2) **Data formatting** <br>\n",
    "   class: `DataFormatter`\n",
    "   \n",
    "   Processes \"query\" feature through CountVectorizer class from scikit-learn, training (70%) and test (30%) sets are generated in sequence.\n",
    "      \n",
    "\n",
    "\n",
    "3) **Modeling** <br>\n",
    "   class: `Modeler`\n",
    "   \n",
    "   Naive Bayes classifier was the chosen model, it's commonly used for testing NLP classification problems. Therefore, the MultinomialNB class from scikit-learn was used, which works well with integer type features generated through CountVectorizer. MultinomialNB is also used for multiple label classification, which is the problem of the present dataset.\n",
    "   \n",
    "\n",
    "\n",
    "4) **Model validation** <br>\n",
    "   class: `ModelValidator`\n",
    "   \n",
    "   Generates metrics about the model accuracy (precision, recall, F1, etc.)\n",
    "   for each category and exports them to a specified path available in the\n",
    "   environment variable `METRICS_PATH`.\n",
    "   \n",
    "   \n",
    "   \n",
    "5) **Model exportation** <br>\n",
    "   class: `ModelExporter`\n",
    "   \n",
    "   Exports a candidate model to a specified path available in the environment variable `MODEL_PATH`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-principle",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "combined-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-injury",
   "metadata": {},
   "source": [
    "# 1. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "speaking-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.path = os.getenv('DATASET_PATH')\n",
    "        self.data = self.get_data()\n",
    "    \n",
    "    def get_data(self):\n",
    "        return pd.read_csv(self.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-portable",
   "metadata": {},
   "source": [
    "#  2. Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fef2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gist.github.com/alopes/5358189\n",
    "stop_words = [ 'a', 'à', 'adeus', 'agora', 'aí', 'ainda', 'além', 'algo', 'alguém', 'algum', 'alguma', 'algumas', 'alguns', 'ali', 'ampla', 'amplas', 'amplo', 'amplos', 'ano', 'anos', 'ante', 'antes', 'ao', 'aos', 'apenas', 'apoio', 'após', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aqui', 'aquilo', 'área', 'as', 'às', 'assim', 'até', 'atrás', 'através', 'baixo', 'bastante', 'bem', 'boa', 'boas', 'bom', 'bons', 'breve', 'cá', 'cada', 'catorze', 'cedo', 'cento', 'certamente', 'certeza', 'cima', 'cinco', 'coisa', 'coisas', 'com', 'como', 'conselho', 'contra', 'contudo', 'custa', 'da', 'dá', 'dão', 'daquela', 'daquelas', 'daquele', 'daqueles', 'dar', 'das', 'de', 'debaixo', 'dela', 'delas', 'dele', 'deles', 'demais', 'dentro', 'depois', 'desde', 'dessa', 'dessas', 'desse', 'desses', 'desta', 'destas', 'deste', 'destes', 'deve', 'devem', 'devendo', 'dever', 'deverá', 'deverão', 'deveria', 'deveriam', 'devia', 'deviam', 'dez', 'dezanove', 'dezasseis', 'dezassete', 'dezoito', 'dia', 'diante', 'disse', 'disso', 'disto', 'dito', 'diz', 'dizem', 'dizer', 'do', 'dois', 'dos', 'doze', 'duas', 'dúvida', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'embora', 'enquanto', 'entre', 'era', 'eram', 'éramos', 'és', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estás', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estiveste', 'estivestes', 'estou', 'etc', 'eu', 'exemplo', 'faço', 'falta', 'favor', 'faz', 'fazeis', 'fazem', 'fazemos', 'fazendo', 'fazer', 'fazes', 'feita', 'feitas', 'feito', 'feitos', 'fez', 'fim', 'final', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'forma', 'formos', 'fosse', 'fossem', 'fôssemos', 'foste', 'fostes', 'fui', 'geral', 'grande', 'grandes', 'grupo', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'havia', 'hei', 'hoje', 'hora', 'horas', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'la', 'lá', 'lado', 'lhe', 'lhes', 'lo', 'local', 'logo', 'longe', 'lugar', 'maior', 'maioria', 'mais', 'mal', 'mas', 'máximo', 'me', 'meio', 'menor', 'menos', 'mês', 'meses', 'mesma', 'mesmas', 'mesmo', 'mesmos', 'meu', 'meus', 'mil', 'minha', 'minhas', 'momento', 'muita', 'muitas', 'muito', 'muitos', 'na', 'nada', 'não', 'naquela', 'naquelas', 'naquele', 'naqueles', 'nas', 'nem', 'nenhum', 'nenhuma', 'nessa', 'nessas', 'nesse', 'nesses', 'nesta', 'nestas', 'neste', 'nestes', 'ninguém', 'nível', 'no', 'noite', 'nome', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'nova', 'novas', 'nove', 'novo', 'novos', 'num', 'numa', 'número', 'nunca', 'o', 'obra', 'obrigada', 'obrigado', 'oitava', 'oitavo', 'oito', 'onde', 'ontem', 'onze', 'os', 'ou', 'outra', 'outras', 'outro', 'outros', 'para', 'parece', 'parte', 'partir', 'paucas', 'pela', 'pelas', 'pelo', 'pelos', 'pequena', 'pequenas', 'pequeno', 'pequenos', 'per', 'perante', 'perto', 'pode', 'pude', 'pôde', 'podem', 'podendo', 'poder', 'poderia', 'poderiam', 'podia', 'podiam', 'põe', 'põem', 'pois', 'ponto', 'pontos', 'por', 'porém', 'porque', 'porquê', 'posição', 'possível', 'possivelmente', 'posso', 'pouca', 'poucas', 'pouco', 'poucos', 'primeira', 'primeiras', 'primeiro', 'primeiros', 'própria', 'próprias', 'próprio', 'próprios', 'próxima', 'próximas', 'próximo', 'próximos', 'pude', 'puderam', 'quais', 'quáis', 'qual', 'quando', 'quanto', 'quantos', 'quarta', 'quarto', 'quatro', 'que', 'quê', 'quem', 'quer', 'quereis', 'querem', 'queremas', 'queres', 'quero', 'questão', 'quinta', 'quinto', 'quinze', 'relação', 'sabe', 'sabem', 'são', 'se', 'segunda', 'segundo', 'sei', 'seis', 'seja', 'sejam', 'sejamos', 'sem', 'sempre', 'sendo', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'sete', 'sétima', 'sétimo', 'seu', 'seus', 'sexta', 'sexto', 'si', 'sido', 'sim', 'sistema', 'só', 'sob', 'sobre', 'sois', 'somos', 'sou', 'sua', 'suas', 'tal', 'talvez', 'também', 'tampouco', 'tanta', 'tantas', 'tanto', 'tão', 'tarde', 'te', 'tem', 'tém', 'têm', 'temos', 'tendes', 'tendo', 'tenha', 'tenham', 'tenhamos', 'tenho', 'tens', 'ter', 'terá', 'terão', 'terceira', 'terceiro', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'ti', 'tido', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tiveste', 'tivestes', 'toda', 'todas', 'todavia', 'todo', 'todos', 'trabalho', 'três', 'treze', 'tu', 'tua', 'tuas', 'tudo', 'última', 'últimas', 'último', 'últimos', 'um', 'uma', 'umas', 'uns', 'vai', 'vais', 'vão', 'vários', 'vem', 'vêm', 'vendo', 'vens', 'ver', 'vez', 'vezes', 'viagem', 'vindo', 'vinte', 'vir', 'você', 'vocês', 'vos', 'vós', 'vossa', 'vossas', 'vosso', 'vossos', 'zero', '1', '2', '3', '4', '5', '6', '7', '8', '9', '0', '_' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "welsh-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFormatter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data = DataExtractor().data\n",
    "    \n",
    "    def data_split(self):\n",
    "        y = self.data['category']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.data['query'], y, test_size=0.3, random_state=123)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    def count_vectorizer(self):\n",
    "        \n",
    "        count_vectorizer = CountVectorizer(lowercase=True, stop_words=stop_words) \n",
    "        count_train = count_vectorizer.fit_transform(self.data_split()[0])\n",
    "        count_test = count_vectorizer.transform(self.data_split()[1])\n",
    "        return count_train, count_test, count_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-gilbert",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intelligent-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modeler:\n",
    "    \n",
    "    def classifier(self):    \n",
    "        nb_classifier = MultinomialNB()\n",
    "        nb_classifier.fit(DataFormatter().count_vectorizer()[0], DataFormatter().data_split()[2])\n",
    "        return nb_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-relaxation",
   "metadata": {},
   "source": [
    "# 4. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "annoying-penalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelValidator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Modeler().classifier()\n",
    "    \n",
    "    def prediction(self):\n",
    "        y_pred = self.model.predict(DataFormatter().count_vectorizer()[1])\n",
    "        return y_pred\n",
    "    \n",
    "    def metrics(self):\n",
    "        metrics_report = classification_report(DataFormatter().data_split()[3].values, self.prediction())\n",
    "        f = open(os.getenv('METRICS_PATH'), 'w')\n",
    "        f.write(f'Test set metrics:\\n')\n",
    "        f.write(f'\\n{metrics_report}\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-fancy",
   "metadata": {},
   "source": [
    "# 5. Data engineering and model exportation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defined-safety",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelExporter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = Modeler().classifier()    \n",
    "        self.data_engineering = DataFormatter().count_vectorizer()[2]\n",
    "        pickle.dump(self.model, open(os.getenv('MODEL_PATH'),'wb'))\n",
    "        pickle.dump(self.data_engineering, open(os.getenv('DATA_ENGINEERING_PATH'),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9e323a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline execution\n",
    "if __name__ == '__main__':\n",
    "    ModelValidator().metrics()\n",
    "    ModelExporter()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
